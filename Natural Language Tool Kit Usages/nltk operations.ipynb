{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.collections.LazySubsequence'> 100\n",
      "[['Assembly', 'session', 'brought', 'much', 'good'], ['The', 'General', 'Assembly', ',', 'which', 'adjourns', 'today', ',', 'has', 'performed', 'in', 'an', 'atmosphere', 'of', 'crisis', 'and', 'struggle', 'from', 'the', 'day', 'it', 'convened', '.'], ...]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "data = brown.sents(categories='editorial')[:100]\n",
    "print(type(data), len(data))\n",
    "print(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline\n",
    "- Data Collection \n",
    "- Tokenization, Stopwards Removal, Stemming\n",
    "- Building a common vocab \n",
    "- Vectorize the documents \n",
    "- Performing Classification/Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization and Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"It was a very pleasant day, the weather was cool and there were showers. I went to market to buy some fruits.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a very pleasant day, the weather was cool and there were showers.', 'I went to market to buy some fruits.']\n"
     ]
    }
   ],
   "source": [
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'a', 'very', 'pleasant', 'day', ',', 'the', 'weather', 'was', 'cool', 'and', 'there', 'were', 'showers', '.']\n"
     ]
    }
   ],
   "source": [
    "word_list = word_tokenize(sents[0].lower())\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'haven', 'this', 'just', 'don', \"weren't\", 'did', 'ma', 'as', 'those', 'ain', 'are', 'didn', 'mightn', 'our', 'you', 'these', \"you've\", \"it's\", 'too', 'being', 'their', \"aren't\", 'he', 'we', 'hers', 'where', 'won', 'with', \"hasn't\", 'themselves', \"haven't\", 'or', 'they', 'against', 'hadn', 'both', 'who', 'll', 'but', 're', 'couldn', 'isn', 'does', 'only', 'why', \"wouldn't\", 's', 'which', 'before', 'into', 'there', \"won't\", 'ours', 'has', 'that', 'how', 'itself', 'off', \"hadn't\", 'most', 'the', 'down', 'not', 'all', 'his', 'them', 'than', 'wouldn', 'if', 'after', 'other', 'yours', 'same', 've', \"mustn't\", 'very', 'can', 'each', 'hasn', 'doing', 'yourself', 'am', 'weren', 'for', 'do', 'my', 'have', \"shouldn't\", 'of', 'above', 'in', \"you'd\", 'mustn', 'such', 'it', 'herself', 'through', \"needn't\", \"wasn't\", 'o', 'what', 'were', 'should', 'nor', 'until', 't', 'doesn', \"should've\", 'him', 'i', 'on', 'aren', 'theirs', 'd', 'about', \"mightn't\", \"didn't\", 'further', 'myself', 'is', 'then', 'will', \"doesn't\", \"shan't\", 'while', 'from', 'few', 'was', 'yourselves', 'over', \"that'll\", 'during', \"don't\", \"you're\", 'below', 'himself', 'because', 'its', 'shouldn', 'no', 'some', \"she's\", 'up', 'her', 'any', 'so', 'me', 'again', 'once', 'more', 'at', 'having', \"isn't\", 'a', 'out', 'by', 'under', 'y', 'needn', 'your', \"you'll\", 'shan', 'ourselves', 'own', 'been', 'and', 'whom', 'wasn', 'an', 'm', \"couldn't\", 'here', 'when', 'be', 'now', 'between', 'had', 'to', 'she'} 179\n"
     ]
    }
   ],
   "source": [
    "print(sw, len(sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the words from the sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(word_list):\n",
    "    \n",
    "    useful_words = [w for w in word_list if w not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pleasant', 'day', ',', 'weather', 'cool', 'showers', '.']\n"
     ]
    }
   ],
   "source": [
    "useful_words = filter_words(word_list)\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[a-zA-Z0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['send', 'the', '50', 'documents', 'to', 'abc', 'def', 'ghi']\n"
     ]
    }
   ],
   "source": [
    "sents = \"send the 50 documents to abc, def, ghi.\"\n",
    "print(tokenizer.tokenize(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "- Process that transforms particular words into root words\n",
    "- jumping, jump, jumps, jumped => jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox was seen jumping over the lazy dog from high wall. Foxes love to make jumps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'was', 'seen', 'jumping', 'over', 'the', 'lazy', 'dog', 'from', 'high', 'wall', 'foxes', 'love', 'to', 'make', 'jumps']\n"
     ]
    }
   ],
   "source": [
    "word_list = tokenizer.tokenize(text.lower())\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Stemmers \n",
    "- Snowball Stemmer (Multilingual)\n",
    "- Porter Stemmer \n",
    "- Lancaster Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"jumped\")\n",
    "ps.stem(\"jumping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"lovely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teen\n",
      "teenag\n"
     ]
    }
   ],
   "source": [
    "ps.stem(\"awesome\")\n",
    "ls = LancasterStemmer()\n",
    "ls.stem(\"awesome\")\n",
    "\n",
    "print(ls.stem(\"teenager\"))\n",
    "print(ps.stem(\"teenager\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cour'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SnowballStemmer('french')\n",
    "ss.stem('courais')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Indian cricket team will win world cup, says caption virat kohli, World cup will be held at India in next year.',\n",
    "    'We will win next Lok Sabha Election, says Indian PM',\n",
    "    'The nobel Rabindranath tagore won the hearts of the people', \n",
    "    'The movie Raazi is an exciting thriller based upon real story'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indian cricket team will win world cup, says caption virat kohli, World cup will be held at India in next year.', 'We will win next Lok Sabha Election, says Indian PM', 'The nobel Rabindranath tagore won the hearts of the people', 'The movie Raazi is an exciting thriller based upon real story']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to convert words into numerical features \n",
    "# Building a common vocabulary and vectorize the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "def myTokenizer(sentence):\n",
    "    words = tokenizer.tokenize(sentence.lower())\n",
    "    return filter_words(words)\n",
    "\n",
    "list_words = myTokenizer(corpus[3])\n",
    "print(len(list_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer,ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1\n",
      " 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0]\n",
      "{'indian': 28, 'cricket': 6, 'team': 70, 'win': 82, 'world': 87, 'cup': 9, 'says': 61, 'caption': 3, 'virat': 79, 'kohli': 32, 'held': 22, 'india': 25, 'next': 41, 'year': 91, 'indian cricket': 29, 'cricket team': 7, 'team win': 71, 'win world': 85, 'world cup': 88, 'cup says': 12, 'says caption': 62, 'caption virat': 4, 'virat kohli': 80, 'kohli world': 33, 'cup held': 10, 'held india': 23, 'india next': 26, 'next year': 44, 'indian cricket team': 30, 'cricket team win': 8, 'team win world': 72, 'win world cup': 86, 'world cup says': 90, 'cup says caption': 13, 'says caption virat': 63, 'caption virat kohli': 5, 'virat kohli world': 81, 'kohli world cup': 34, 'world cup held': 89, 'cup held india': 11, 'held india next': 24, 'india next year': 27, 'lok': 35, 'sabha': 58, 'election': 14, 'pm': 49, 'win next': 83, 'next lok': 42, 'lok sabha': 36, 'sabha election': 59, 'election says': 15, 'says indian': 64, 'indian pm': 31, 'win next lok': 84, 'next lok sabha': 43, 'lok sabha election': 37, 'sabha election says': 60, 'election says indian': 16, 'says indian pm': 65, 'nobel': 45, 'rabindranath': 53, 'tagore': 67, 'hearts': 20, 'people': 48, 'nobel rabindranath': 46, 'rabindranath tagore': 54, 'tagore hearts': 68, 'hearts people': 21, 'nobel rabindranath tagore': 47, 'rabindranath tagore hearts': 55, 'tagore hearts people': 69, 'movie': 38, 'raazi': 50, 'exciting': 17, 'thriller': 73, 'based': 0, 'upon': 76, 'real': 56, 'story': 66, 'movie raazi': 39, 'raazi exciting': 51, 'exciting thriller': 18, 'thriller based': 74, 'based upon': 1, 'upon real': 77, 'real story': 57, 'movie raazi exciting': 40, 'raazi exciting thriller': 52, 'exciting thriller based': 19, 'thriller based upon': 75, 'based upon real': 2, 'upon real story': 78}\n"
     ]
    }
   ],
   "source": [
    "print(vc[1])\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['caption', 'cricket', 'cup', 'held', 'india', 'indian', 'kohli',\n",
       "        'next', 'says', 'team', 'virat', 'win', 'world', 'year'],\n",
       "       dtype='<U12')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(vc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=myTokenizer, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.1678685  0.1678685  0.1678685  0.1678685\n",
      "  0.335737   0.1678685  0.1678685  0.         0.         0.\n",
      "  0.         0.         0.         0.1678685  0.1678685  0.1678685\n",
      "  0.1678685  0.13234945 0.1678685  0.         0.1678685  0.1678685\n",
      "  0.         0.         0.         0.         0.13234945 0.\n",
      "  0.1678685  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.13234945 0.1678685  0.         0.         0.\n",
      "  0.         0.1678685  0.1678685  0.         0.         0.\n",
      "  0.         0.1678685  0.1678685  0.13234945 0.         0.1678685\n",
      "  0.335737   0.335737   0.1678685 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.27230302 0.27230302 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.21468683 0.         0.27230302 0.         0.\n",
      "  0.27230302 0.27230302 0.         0.         0.21468683 0.27230302\n",
      "  0.         0.         0.         0.         0.27230302 0.\n",
      "  0.         0.         0.         0.         0.         0.27230302\n",
      "  0.27230302 0.21468683 0.         0.27230302 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.21468683 0.27230302 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.33333333 0.33333333 0.         0.\n",
      "  0.         0.33333333 0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.25819889 0.25819889 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25819889\n",
      "  0.25819889 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.25819889 0.25819889 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25819889\n",
      "  0.25819889 0.         0.         0.25819889 0.25819889 0.\n",
      "  0.         0.         0.         0.         0.25819889 0.\n",
      "  0.         0.         0.         0.25819889 0.25819889 0.25819889\n",
      "  0.25819889 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "{'indian': 19, 'cricket': 4, 'team': 49, 'win': 57, 'world': 60, 'cup': 6, 'says': 43, 'caption': 2, 'virat': 55, 'kohli': 22, 'held': 15, 'india': 17, 'next': 28, 'year': 62, 'indian cricket': 20, 'cricket team': 5, 'team win': 50, 'win world': 59, 'world cup': 61, 'cup says': 8, 'says caption': 44, 'caption virat': 3, 'virat kohli': 56, 'kohli world': 23, 'cup held': 7, 'held india': 16, 'india next': 18, 'next year': 30, 'lok': 24, 'sabha': 41, 'election': 9, 'pm': 34, 'win next': 58, 'next lok': 29, 'lok sabha': 25, 'sabha election': 42, 'election says': 10, 'says indian': 45, 'indian pm': 21, 'nobel': 31, 'rabindranath': 37, 'tagore': 47, 'hearts': 13, 'people': 33, 'nobel rabindranath': 32, 'rabindranath tagore': 38, 'tagore hearts': 48, 'hearts people': 14, 'movie': 26, 'raazi': 35, 'exciting': 11, 'thriller': 51, 'based': 0, 'upon': 53, 'real': 39, 'story': 46, 'movie raazi': 27, 'raazi exciting': 36, 'exciting thriller': 12, 'thriller based': 52, 'based upon': 1, 'upon real': 54, 'real story': 40}\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "print(vectorized_corpus)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
